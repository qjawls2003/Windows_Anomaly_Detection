# Windows Anomaly Detection for Processes

## Idea:
Utilize the latest natural language processing techniques to identify anomalous processes/commands executed on Windows host.
1. Collect Windows Event 4688 and ingest them to SIEM
2. Export data as a csv and feed to pre-processing
3. Feed the pre-processed data to word embedding model called WordPiece
4. Once the words are tokenized, feed the pre-processed dataset and the token dictionary to to BERT training model.
5. Setting up the BERT trainer, Masked Language Model is used to mask 15% of the data by replacing the words with a mask token.
6. Specically, RoBERTa model is utilized to optimize the model performance.
7. Lastly, test the data by inputting a new event data and masking its process name. If the model returns with a <mask> token as the highest score, then the event data is anomlous.


## Data Collection

1. Windows Event Logs generated by Sysmon and others
2. ElasticAgent forwards all Windows Event Logs to SIEM (ELK)
3. On Kibana, goto Home -> Analytics -> Discover
4. Create a tabular dashboard to filter Windows Event ID 4688 (Process Creation)
5. Columns: @timestamp, host.name, process.name, process.parent.name, process.pid, process.command_line, user.name, user.id, user.domain.
6. Download .csv file (pre_processing/data/WinEvent4688.csv)

## Pre-processing

1. Convert CSV to python pandas dataframe (pre_processing/src/csv_to_df.py)
2. Extract columns and create a list of words to minic a sentence
3. Input the sentences to word embedding module (word2vec, etc.) (pre_processing/src/vectorize.py)
4. Prepare each sentence as a single string for tokenizer

## Tokenizer
1. Initialize Tokenizer model with WordPiece algorithm
2. Initialize the trainer with hyperparameters such as vocab_size and min_frequency
3. Input the dataset to tokenize all sentences 
4. Outputs the token ids of all words in .json
5. Using the Tokenizer, encode the dataset and outputs the ids of each word in the sentences and also the attention masks.

## Modeling
1. Install PyTorch and create a conda env to run cuda supported Pytorch.
2. Prepare tensors to train the model
3. Train the model

```
bazel run //:main
```

# TODO

#### Modeling
- ~~Get PyTorch CUDA using NVIDIA GPU working~~ Using conda
- ``Complete layers``
- Test out different hyperparameters for training the models and customize layers
  
#### Scoring and Anomaly Detection
- Improve scoring method by aggregating multiple techniques

#### Embedding
- ~~Figure out which word embedding to use~~ Using WordPiece
- ~~Figure out what the multi-dimensional vectors really mean~~
- ``How to detect anomaly using the vectors?``
- ~~What if the new commands have words not in the embedded model? How do we deal with new words not in the dictionary?~~ Masking Language Model

#### Clustering?
- PCA for dimension reductions?
- Multiple clusters based time (month, day, hour), process name, parent process name, user, domain: assign an cluster id for each log entry and the use word embedding on the command lines

#### More pre-processing
- ~~How to formulate a sentence so that all features are added (time, users, parent processes, and ultimately command line words)~~
- ~~Generalize @timestamp to "month, day of the week, and day/night"~~
